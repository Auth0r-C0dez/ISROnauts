{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Environment setup\n"
      ],
      "metadata": {
        "id": "grrV0JsVQiww"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUypNSaJ2UyC",
        "outputId": "7ceaaa97-6992-4ee3-9f2d-5f2185111041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.6/962.6 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Cell 1: Mount Drive & Install Dependencies\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Install required libraries\n",
        "!pip install --quiet torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install --quiet xarray h5py einops tqdm scikit-image opencv-python matplotlib pytorch-lightning\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Define Dataset Path and Configuration (for .h5-based dataset)\n",
        "\n",
        "import os\n",
        "\n",
        "# Path to the folder with .h5 files\n",
        "DATASET_DIR = '/content/drive/MyDrive/reshaped_DS'\n",
        "\n",
        "# Prototype configuration\n",
        "IMG_SIZE = (64, 64)         # Resized shape to downsample images\n",
        "NUM_INPUT_FRAMES = 6        # Past 3 hours (30 min interval)\n",
        "NUM_OUTPUT_FRAMES = 2       # Future 1 hour (30 min intervals)\n",
        "CHANNELS = 1                # Single spectral band (e.g., IR)\n",
        "\n",
        "# File format\n",
        "FILE_EXTENSION = '.h5'      # We'll scan for HDF5 files\n",
        "\n",
        "# List and sort files\n",
        "all_files = sorted([\n",
        "    f for f in os.listdir(DATASET_DIR)\n",
        "    if f.endswith(FILE_EXTENSION)\n",
        "])\n",
        "\n",
        "# Quick check\n",
        "print(f\"Total HDF5 files found: {len(all_files)}\")\n",
        "print(\"Sample file names:\", all_files[:3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-hFmBxrRJRM",
        "outputId": "3502f9aa-38db-4107-f963-6dbc6cd83b19"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total HDF5 files found: 9\n",
            "Sample file names: ['3SIMG_24JUN2025_0000_L1C_SGP_V01R00.h5', '3SIMG_24JUN2025_0030_L1C_SGP_V01R00.h5', '3SIMG_24JUN2025_0100_L1C_SGP_V01R00.h5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load and Prepare a Sample Sequence (Input: 6, Output: 2) from .h5 Files\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def read_band_from_h5(filepath, band_key='IMG_TIR1'):\n",
        "    with h5py.File(filepath, 'r') as f:\n",
        "        band = f[band_key][0]  # shape: (128, 128)\n",
        "    return band\n",
        "\n",
        "def normalize_band(band_array):\n",
        "    return band_array.astype(np.float32) / 255.0  # normalize to [0, 1]\n",
        "\n",
        "def load_sequence(file_list, band_key='IMG_TIR1'):\n",
        "    \"\"\"\n",
        "    Loads 6 input frames and 2 output frames using a single band.\n",
        "    Returns tensors: input_tensor (6 frames), target_tensor (2 frames)\n",
        "    \"\"\"\n",
        "    input_frames = []\n",
        "    target_frames = []\n",
        "\n",
        "    for i, filename in enumerate(file_list[:NUM_INPUT_FRAMES + NUM_OUTPUT_FRAMES]):\n",
        "        filepath = os.path.join(DATASET_DIR, filename)\n",
        "        band = read_band_from_h5(filepath, band_key)\n",
        "        band_norm = normalize_band(band)\n",
        "\n",
        "        if i < NUM_INPUT_FRAMES:\n",
        "            input_frames.append(band_norm)\n",
        "        else:\n",
        "            target_frames.append(band_norm)\n",
        "\n",
        "    # Convert to tensors: shape (1, 1, T, H, W)\n",
        "    input_tensor = torch.tensor(np.stack(input_frames)).unsqueeze(0).unsqueeze(0)\n",
        "    target_tensor = torch.tensor(np.stack(target_frames)).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    print(f\"Input tensor shape: {input_tensor.shape}\")    # (1, 1, 6, 128, 128)\n",
        "    print(f\"Target tensor shape: {target_tensor.shape}\")  # (1, 1, 2, 128, 128)\n",
        "\n",
        "    return input_tensor, target_tensor\n",
        "\n",
        "# Try loading the first 8 files (6 for input + 2 for output)\n",
        "input_tensor, target_tensor = load_sequence(all_files, band_key='IMG_TIR1')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFhapknFTiZj",
        "outputId": "2f19c358-55d9-4678-9e88-6cbd70568966"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor shape: torch.Size([1, 1, 6, 128, 128])\n",
            "Target tensor shape: torch.Size([1, 1, 2, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Corrected 3D UNet that outputs only NUM_OUTPUT_FRAMES frames\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet3D(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, base_channels=16):\n",
        "        super(UNet3D, self).__init__()\n",
        "\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, base_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(base_channels, base_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=2)\n",
        "\n",
        "        self.encoder2 = nn.Sequential(\n",
        "            nn.Conv3d(base_channels, base_channels * 2, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(base_channels * 2, base_channels * 2, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=2)\n",
        "\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv3d(base_channels * 2, base_channels * 4, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.decoder2 = nn.Sequential(\n",
        "            nn.Conv3d(base_channels * 4, base_channels * 2, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder1 = nn.Sequential(\n",
        "            nn.Conv3d(base_channels * 2, base_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.final_conv = nn.Conv3d(base_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)                     # -> (B, C, T, H, W)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))      # -> (B, C2, T//2, H//2, W//2)\n",
        "        bottleneck = self.bottleneck(self.pool2(enc2))  # -> (B, C4, T//4, H//4, W//4)\n",
        "\n",
        "        # Upsample to enc2 shape\n",
        "        up2 = F.interpolate(bottleneck, size=enc2.shape[2:], mode='trilinear', align_corners=False)\n",
        "        dec2 = self.decoder2(up2)\n",
        "\n",
        "        # Upsample to enc1 shape\n",
        "        up1 = F.interpolate(dec2, size=enc1.shape[2:], mode='trilinear', align_corners=False)\n",
        "        dec1 = self.decoder1(up1)\n",
        "\n",
        "        out = self.final_conv(dec1)  # -> (B, 1, T, H, W)\n",
        "\n",
        "        # Slice temporal dimension to keep only the next NUM_OUTPUT_FRAMES frames\n",
        "        # Assumes global NUM_OUTPUT_FRAMES is defined (e.g., 2)\n",
        "        out = out[:, :, -NUM_OUTPUT_FRAMES:, :, :]\n",
        "\n",
        "        return out\n",
        "\n",
        "# Instantiate the model\n",
        "model = UNet3D(in_channels=1, out_channels=1)\n",
        "print(f\"Model initialized. Total parameters: {sum(p.numel() for p in model.parameters())}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8640g48oUSM5",
        "outputId": "6331c5a2-6b63-4624-b3a3-417503e5b30a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized. Total parameters: 173457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Training Setup and Mini Training Loop (Fixed for Updated UNet3D)\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Move model and data to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "input_tensor = input_tensor.to(device)\n",
        "target_tensor = target_tensor.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Number of training epochs for prototype\n",
        "EPOCHS = 10\n",
        "\n",
        "print(\"Starting training...\\n\")\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(input_tensor)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(output, target_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Loss: {loss.item():.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvUwhenVUnN5",
        "outputId": "0252f592-9aa6-46ad-fa1e-8149234db1f0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "\n",
            "Epoch [1/10] - Loss: 6.867555\n",
            "Epoch [2/10] - Loss: 6.777252\n",
            "Epoch [3/10] - Loss: 6.505719\n",
            "Epoch [4/10] - Loss: 5.526911\n",
            "Epoch [5/10] - Loss: 3.057830\n",
            "Epoch [6/10] - Loss: 0.598686\n",
            "Epoch [7/10] - Loss: 5.179178\n",
            "Epoch [8/10] - Loss: 0.521489\n",
            "Epoch [9/10] - Loss: 1.336145\n",
            "Epoch [10/10] - Loss: 2.498595\n"
          ]
        }
      ]
    }
  ]
}